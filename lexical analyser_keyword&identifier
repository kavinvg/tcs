#to implement lexical analyser programs (keywords and identifier)
import re

token_patterns = [
    ('KEYWORD', r'(if|else|while|for|return|int|float|char|void)'),
    ('IDENTIFIER', r'[a-zA-Z_][a-zA-Z0-9_]*'),
    ('WHITESPACE', r'\s+'),
]

token_regex = '|'.join('(?P<{}>{})'.format(name, pattern) for name, pattern in token_patterns)

def tokenize(text):
    tokens = []
    for match in re.finditer(token_regex, text):
        token_type = match.lastgroup
        token_value = match.group(token_type)
        if token_type != 'WHITESPACE':
           tokens.append((token_type, token_value))
    return tokens

text = input("Enter a code snippet: ")

tokens = tokenize(text)
print("Tokens:", tokens)
