#To implement Lexical Analyzer programs
import re
token_patterns = [
    ('NUMBER', r'\d+'),
    ('PLUS', r'\+'),
    ('MINUS', r'-'),
    ('TIMES', r'\*'),
    ('DIVIDE', r'/'),
    ('LPAREN', r'\('),
    ('RPAREN', r'\)'),
    ('WHITESPACE', r'\s+'),
]
token_regex = '|'.join('(?P<{}>{})'.format(name, pattern) for name, pattern in token_patterns)

def tokenize(text):
    tokens = []
    for match in re.finditer(token_regex, text):
        token_type = match.lastgroup
        token_value = match.group(token_type)
        if token_type != 'WHITESPACE':
            tokens.append((token_type, token_value))
    return tokens
text = input("Enter an expression: ")

tokens = tokenize(text)
print("Tokens:", tokens)
